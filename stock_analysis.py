"""
å°è‚¡æ–°èç†±é–€è‚¡ç¥¨åˆ†æç³»çµ± - GitHub Actions æ”¹è‰¯ç‰ˆ
å¢åŠ è©³ç´°æ—¥èªŒã€éŒ¯èª¤è™•ç†å’Œè¶…æ™‚æ§åˆ¶
"""

import os
import sys
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import json
import pytz
import re
import pandas as pd
from collections import defaultdict
import time
import traceback

# ========== åŸ·è¡Œè¨­å®š ==========
PROCESS_MODE = os.environ.get('PROCESS_MODE', 'TSE')  # 'TSE', 'OTC', 'BOTH'
TW_TZ = pytz.timezone('Asia/Taipei')

# è¶…æ™‚è¨­å®š
REQUEST_TIMEOUT = 10  # å–®æ¬¡è«‹æ±‚è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰
MAX_RETRY = 3  # æœ€å¤§é‡è©¦æ¬¡æ•¸
STOCK_DELAY = 1.5  # è‚¡ç¥¨æŸ¥è©¢é–“éš”ï¼ˆç§’ï¼‰

# ========== è·¯å¾‘è¨­å®š ==========
BASE_PATH = os.path.join(os.path.dirname(__file__), 'StockInfo')

# ç¢ºä¿ StockInfo è³‡æ–™å¤¾å­˜åœ¨
if not os.path.exists(BASE_PATH):
    os.makedirs(BASE_PATH)
    print(f"âœ… å‰µå»ºè³‡æ–™å¤¾: {BASE_PATH}")

NEWS_JSON = os.path.join(BASE_PATH, 'twstock_news.json')
TSE_RANKING = os.path.join(BASE_PATH, 'TSE_buy_ranking.txt')
OTC_RANKING = os.path.join(BASE_PATH, 'OTC_buy_ranking.txt')
TSE_OUTPUT_JSON = os.path.join(BASE_PATH, 'TSE_hotstock_data.json')
OTC_OUTPUT_JSON = os.path.join(BASE_PATH, 'OTC_hotstock_data.json')
TSE_CSV = os.path.join(BASE_PATH, 'tse_company_list.csv')
OTC_CSV = os.path.join(BASE_PATH, 'otc_company_list.csv')

# ========== æ—¥èªŒå‡½æ•¸ ==========
def log_info(message):
    """è¼¸å‡ºè³‡è¨Šæ—¥èªŒ"""
    timestamp = datetime.now(TW_TZ).strftime('%H:%M:%S')
    print(f"[{timestamp}] â„¹ï¸  {message}")
    sys.stdout.flush()

def log_success(message):
    """è¼¸å‡ºæˆåŠŸæ—¥èªŒ"""
    timestamp = datetime.now(TW_TZ).strftime('%H:%M:%S')
    print(f"[{timestamp}] âœ… {message}")
    sys.stdout.flush()

def log_warning(message):
    """è¼¸å‡ºè­¦å‘Šæ—¥èªŒ"""
    timestamp = datetime.now(TW_TZ).strftime('%H:%M:%S')
    print(f"[{timestamp}] âš ï¸  {message}")
    sys.stdout.flush()

def log_error(message):
    """è¼¸å‡ºéŒ¯èª¤æ—¥èªŒ"""
    timestamp = datetime.now(TW_TZ).strftime('%H:%M:%S')
    print(f"[{timestamp}] âŒ {message}")
    sys.stdout.flush()

def log_debug(message):
    """è¼¸å‡ºé™¤éŒ¯æ—¥èªŒ"""
    timestamp = datetime.now(TW_TZ).strftime('%H:%M:%S')
    print(f"[{timestamp}] ğŸ” {message}")
    sys.stdout.flush()

# ========== æª”æ¡ˆæª¢æŸ¥å‡½æ•¸ ==========
def check_required_files():
    """æª¢æŸ¥å¿…è¦æª”æ¡ˆæ˜¯å¦å­˜åœ¨"""
    log_info("æª¢æŸ¥å¿…è¦æª”æ¡ˆ...")
    
    required_files = {
        'TSE CSV': TSE_CSV,
        'OTC CSV': OTC_CSV,
        'TSE è²·è¶…': TSE_RANKING,
        'OTC è²·è¶…': OTC_RANKING
    }
    
    missing_files = []
    for name, filepath in required_files.items():
        if os.path.exists(filepath):
            size = os.path.getsize(filepath)
            log_success(f"{name}: å­˜åœ¨ ({size} bytes)")
        else:
            log_error(f"{name}: ä¸å­˜åœ¨ - {filepath}")
            missing_files.append(name)
    
    if missing_files:
        log_error(f"ç¼ºå°‘å¿…è¦æª”æ¡ˆ: {', '.join(missing_files)}")
        log_info("è«‹åƒè€ƒ SAMPLE_DATA_FORMAT.md æº–å‚™è³‡æ–™æª”æ¡ˆ")
        return False
    
    log_success("æ‰€æœ‰å¿…è¦æª”æ¡ˆæª¢æŸ¥é€šé!")
    return True

# ========== æ–°èçˆ¬èŸ²å‡½æ•¸ ==========
def clean_title(title_text):
    """æ¸…ç†æ–°èæ¨™é¡Œ"""
    if not title_text:
        return ""
    title = title_text.strip()
    if title.startswith('å‰å¾€'):
        title = title[2:]
    title = title.replace('ã€å³æ™‚æ–°èã€‘', '')
    if title.endswith('æ–‡ç« é '):
        title = title[:-3]
    patterns_to_remove = [r'^å‰å¾€', r'æ–‡ç« é $', r'^æŸ¥çœ‹', r'é»æ“ŠæŸ¥çœ‹$', r'^é–±è®€']
    for pattern in patterns_to_remove:
        title = re.sub(pattern, '', title)
    title = ' '.join(title.split())
    return title.strip()

def parse_publish_time(soup_element):
    """è§£ææ–°èç™¼å¸ƒæ™‚é–“"""
    try:
        time_patterns = [
            soup_element.find('time'),
            soup_element.find(class_=re.compile('time|date|published')),
            soup_element.find('span', class_=re.compile('time|date')),
        ]
        for time_elem in time_patterns:
            if time_elem:
                if time_elem.get('datetime'):
                    return time_elem.get('datetime')
                time_text = time_elem.get_text(strip=True)
                if time_text:
                    return time_text
        return None
    except Exception as e:
        log_debug(f"è§£ææ™‚é–“å¤±æ•—: {e}")
        return None

def scrape_twstock_news():
    """çˆ¬å–å°è‚¡æ–°è"""
    url = "https://cmnews.com.tw/twstock/twstock_news"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'zh-TW,zh;q=0.9',
    }
    
    for attempt in range(MAX_RETRY):
        try:
            log_info(f"å˜—è©¦æŠ“å–æ–°è (ç¬¬ {attempt + 1}/{MAX_RETRY} æ¬¡)...")
            response = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                log_warning(f"HTTP ç‹€æ…‹ç¢¼: {response.status_code}")
                if attempt < MAX_RETRY - 1:
                    time.sleep(2)
                    continue
                return []
            
            soup = BeautifulSoup(response.text, 'lxml')
            current_time = datetime.now(TW_TZ)
            news_list = []
            seen_urls = set()
            
            articles = soup.find_all('a', href=lambda x: x and '/article/' in x)
            log_info(f"æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« é€£çµ")
            
            for article in articles:
                news_url = article.get('href', '')
                if news_url and not news_url.startswith('http'):
                    news_url = 'https://cmnews.com.tw' + news_url
                if news_url in seen_urls:
                    continue
                title_text = article.get_text(strip=True)
                clean_title_text = clean_title(title_text)
                if clean_title_text and len(clean_title_text) > 5:
                    seen_urls.add(news_url)
                    publish_time = parse_publish_time(article.parent if article.parent else article)
                    news_item = {
                        'title': clean_title_text,
                        'url': news_url,
                        'publish_time': publish_time if publish_time else "æœªçŸ¥",
                        'scraped_time': current_time.strftime('%Y-%m-%d %H:%M:%S'),
                        'timezone': 'Asia/Taipei'
                    }
                    news_list.append(news_item)
            
            log_success(f"æˆåŠŸæŠ“å– {len(news_list)} å‰‡æ–°è")
            return news_list
            
        except requests.Timeout:
            log_warning(f"è«‹æ±‚è¶…æ™‚ (ç¬¬ {attempt + 1} æ¬¡)")
            if attempt < MAX_RETRY - 1:
                time.sleep(2)
        except Exception as e:
            log_error(f"æŠ“å–æ–°èå¤±æ•—: {e}")
            log_debug(traceback.format_exc())
            if attempt < MAX_RETRY - 1:
                time.sleep(2)
    
    log_warning("é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œä½¿ç”¨ç¾æœ‰æ–°èè³‡æ–™")
    return []

def load_existing_news(filepath):
    """è¼‰å…¥ç¾æœ‰æ–°è"""
    if os.path.exists(filepath):
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
                log_info(f"è¼‰å…¥ç¾æœ‰æ–°è: {len(data)} å‰‡")
                return data
        except Exception as e:
            log_warning(f"è¼‰å…¥ç¾æœ‰æ–°èå¤±æ•—: {e}")
            return []
    log_info("æ²’æœ‰ç¾æœ‰æ–°èè³‡æ–™")
    return []

def filter_news_by_time(news_list, hours=24):
    """éæ¿¾æŒ‡å®šæ™‚é–“å…§çš„æ–°è"""
    current_time = datetime.now(TW_TZ)
    time_limit = current_time - timedelta(hours=hours)
    filtered_news = []
    for news in news_list:
        try:
            news_time_str = news['scraped_time']
            news_time = datetime.strptime(news_time_str, '%Y-%m-%d %H:%M:%S')
            news_time = TW_TZ.localize(news_time)
            if news_time >= time_limit:
                filtered_news.append(news)
        except:
            filtered_news.append(news)
    log_info(f"éæ¿¾å¾Œä¿ç•™ {len(filtered_news)} å‰‡ (24å°æ™‚å…§)")
    return filtered_news

def merge_news(existing_news, new_news):
    """åˆä½µæ–°èˆŠæ–°è"""
    news_dict = {}
    for news in existing_news:
        news_dict[news['url']] = news
    for news in new_news:
        news_dict[news['url']] = news
    merged = list(news_dict.values())
    merged.sort(key=lambda x: x['scraped_time'], reverse=True)
    log_info(f"åˆä½µå¾Œå…± {len(merged)} å‰‡æ–°è")
    return merged

def save_news_to_json(news_list, filepath):
    """å„²å­˜æ–°èåˆ° JSON"""
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(news_list, f, ensure_ascii=False, indent=2)
        log_success(f"å„²å­˜æ–°èæˆåŠŸ: {filepath}")
    except Exception as e:
        log_error(f"å„²å­˜æ–°èå¤±æ•—: {e}")

# ========== è‚¡ç¥¨æ¸…å–®è¼‰å…¥ ==========
def load_stock_list(filepath, market_type):
    """è¼‰å…¥è‚¡ç¥¨æ¸…å–®"""
    try:
        log_info(f"è¼‰å…¥ {market_type} è‚¡ç¥¨æ¸…å–®: {filepath}")
        df = pd.read_csv(filepath, encoding='utf-8-sig', header=None)
        stock_dict = {}
        for _, row in df.iterrows():
            code = str(row[0]).strip()
            name = str(row[1]).strip()
            industry = str(row[2]).strip() if len(row) > 2 else ""
            name_clean = re.sub(r'\s+', '', name)
            stock_dict[name_clean] = {
                'code': code,
                'name': name,
                'industry': industry,
                'market': market_type
            }
        log_success(f"{market_type} è‚¡ç¥¨æ¸…å–®: {len(stock_dict)} æª”")
        return stock_dict
    except Exception as e:
        log_error(f"è¼‰å…¥ {market_type} å¤±æ•—: {e}")
        log_debug(traceback.format_exc())
        return {}

def analyze_news_stocks(news_list, tse_stocks, otc_stocks):
    """åˆ†ææ–°èä¸­æåŠçš„è‚¡ç¥¨"""
    log_info("åˆ†ææ–°èä¸­çš„è‚¡ç¥¨...")
    all_stocks = {}
    all_stocks.update(tse_stocks)
    all_stocks.update(otc_stocks)
    stock_mentions = defaultdict(lambda: {'count': 0, 'news': []})
    
    for news in news_list:
        title = news.get('title', '')
        url = news.get('url', '')
        title_clean = re.sub(r'\s+', '', title)
        for stock_name, stock_info in all_stocks.items():
            if stock_name in title_clean or stock_info['code'] in title:
                stock_mentions[stock_name]['count'] += 1
                stock_mentions[stock_name]['info'] = stock_info
                stock_mentions[stock_name]['news'].append({
                    'title': title,
                    'url': url,
                    'time': news.get('scraped_time', '')
                })
    
    log_success(f"æ‰¾åˆ° {len(stock_mentions)} æª”è¢«æåŠçš„è‚¡ç¥¨")
    return stock_mentions

# ========== è²·è¶…æ’è¡Œæ¦œè¼‰å…¥ ==========
def load_buy_ranking(filepath):
    """è¼‰å…¥è²·è¶…æ’è¡Œæ¦œ"""
    buy_ranking = {}
    if not os.path.exists(filepath):
        log_warning(f"æ‰¾ä¸åˆ°æª”æ¡ˆ: {filepath}")
        return buy_ranking
    try:
        log_info(f"è¼‰å…¥è²·è¶…æ’è¡Œ: {filepath}")
        with open(filepath, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        for line in lines:
            line = line.strip()
            if line.startswith('#') or not line:
                continue
            parts = line.split(',')
            if len(parts) >= 4:
                code = parts[1].strip()
                name = parts[2].strip()
                buy_volume = parts[3].strip()
                try:
                    buy_volume_int = int(buy_volume)
                    buy_ranking[code] = {
                        'name': name,
                        'volume': buy_volume_int
                    }
                except:
                    pass
        log_success(f"è¼‰å…¥è²·è¶…æ’è¡Œ: {len(buy_ranking)} æª”")
        return buy_ranking
    except Exception as e:
        log_error(f"è¼‰å…¥è²·è¶…æ’è¡Œæ¦œå¤±æ•—: {e}")
        log_debug(traceback.format_exc())
        return buy_ranking

def merge_stocks(news_stocks, buy_ranking, stock_list, market_type):
    """åˆä½µæ–°èè‚¡ç¥¨å’Œè²·è¶…è‚¡ç¥¨"""
    log_info(f"åˆä½µ {market_type} å¸‚å ´è‚¡ç¥¨...")
    merged = {}
    
    # åŠ å…¥æ–°èè‚¡ç¥¨
    for stock_name, data in news_stocks.items():
        if data['info']['market'] == market_type:
            code = data['info']['code']
            buy_info = buy_ranking.get(code, {})
            buy_volume = buy_info.get('volume', 0)
            
            merged[code] = {
                'code': code,
                'name': data['info']['name'],
                'market': market_type,
                'mention_count': data['count'],
                'yesterday_buy': buy_volume,
                'source': 'news'
            }
    
    # åŠ å…¥è²·è¶…æ’è¡Œè‚¡ç¥¨
    for code, buy_info in buy_ranking.items():
        buy_volume = buy_info['volume']
        if code not in merged:
            found = False
            for stock_name, stock_info in stock_list.items():
                if stock_info['code'] == code and stock_info['market'] == market_type:
                    merged[code] = {
                        'code': code,
                        'name': stock_info['name'],
                        'market': market_type,
                        'mention_count': 0,
                        'yesterday_buy': buy_volume,
                        'source': 'buy'
                    }
                    found = True
                    break
            if not found:
                merged[code] = {
                    'code': code,
                    'name': buy_info['name'],
                    'market': market_type,
                    'mention_count': 0,
                    'yesterday_buy': buy_volume,
                    'source': 'buy'
                }
        else:
            merged[code]['yesterday_buy'] = buy_volume
    
    log_success(f"{market_type} åˆä½µå¾Œ: {len(merged)} æª”")
    return merged

# ========== Yahoo è‚¡åƒ¹çˆ¬èŸ² ==========
def get_stock_info(stock_code, market):
    """æŠ“å–è‚¡ç¥¨å³æ™‚è³‡è¨Š"""
    suffix = '.TW' if market == 'TSE' else '.TWO'
    yahoo_code = f"{stock_code}{suffix}"
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
    url = f'https://tw.stock.yahoo.com/quote/{yahoo_code}'
    
    for attempt in range(MAX_RETRY):
        try:
            response = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            stock_info = {'è‚¡ç¥¨ä»£ç¢¼': stock_code, 'å¸‚å ´': market}
            price_items = soup.find_all('li', class_=re.compile(r'price-detail-item'))
            field_map = {'é–‹ç›¤': 'é–‹ç›¤åƒ¹', 'æ¼²è·Œå¹…': 'æ¼²è·Œå¹…', 'æ¼²è·Œ': 'æ¼²è·Œ'}
            stock_info['æˆäº¤åƒ¹'] = 'ç„¡è³‡æ–™'
            for field in field_map.values():
                stock_info[field] = 'ç„¡è³‡æ–™'
            for item in price_items:
                item_text = item.get_text().strip()
                if item_text.startswith('æˆäº¤') and 'é‡‘é¡' not in item_text and 'é‡' not in item_text:
                    value = item_text[2:].strip().replace(',', '')
                    stock_info['æˆäº¤åƒ¹'] = value
                    continue
                for keyword, field_name in field_map.items():
                    if item_text.startswith(keyword):
                        value = item_text[len(keyword):].strip().replace(',', '')
                        if field_name in ['æ¼²è·Œ', 'æ¼²è·Œå¹…'] and value != 'ç„¡è³‡æ–™' and value:
                            all_spans = item.find_all('span')
                            is_down = is_up = False
                            for span in all_spans:
                                span_class = ' '.join(span.get('class', []))
                                if 'trend-down' in span_class:
                                    is_down = True
                                    break
                                elif 'trend-up' in span_class:
                                    is_up = True
                                    break
                            if is_down:
                                value = f"-{value}" if not value.startswith('-') else value
                            elif is_up:
                                value = f"+{value}" if not value.startswith('+') else value
                            else:
                                if '-' not in value and '+' not in value:
                                    value = f"+{value}"
                        stock_info[field_name] = value
                        break
            buy_total = sell_total = 'ç„¡è³‡æ–™'
            all_divs = soup.find_all('div', class_=True)
            for div in all_divs:
                class_str = ' '.join(div.get('class', []))
                div_text = div.get_text()
                if 'Mend(16px)' in class_str and 'å°è¨ˆ' in div_text:
                    match = re.search(r'([\d,]+)\s*å°è¨ˆ', div_text)
                    if match:
                        buy_total = match.group(1).replace(',', '')
                if 'Mstart(16px)' in class_str and 'Mend(0)' in class_str and 'å°è¨ˆ' in div_text:
                    match = re.search(r'å°è¨ˆ\s*([\d,]+)', div_text)
                    if match:
                        sell_total = match.group(1).replace(',', '')
            stock_info['å§”è²·å°è¨ˆ'] = buy_total
            stock_info['å§”è³£å°è¨ˆ'] = sell_total
            return stock_info
        except requests.Timeout:
            if attempt < MAX_RETRY - 1:
                time.sleep(1)
            continue
        except Exception as e:
            if attempt < MAX_RETRY - 1:
                time.sleep(1)
            continue
    
    return None

def fetch_stocks_price(stock_list, delay=1.5):
    """æ‰¹æ¬¡æŠ“å–è‚¡ç¥¨è³‡è¨Š"""
    results = []
    total = len(stock_list)
    log_info(f"é–‹å§‹æŠ“å– {total} æª”è‚¡ç¥¨è³‡è¨Š...")
    
    for i, stock_data in enumerate(stock_list, 1):
        code = stock_data['code']
        name = stock_data['name']
        market = stock_data['market']
        
        # æ¯ 10 æª”é¡¯ç¤ºä¸€æ¬¡é€²åº¦
        if i % 10 == 0 or i == total:
            log_info(f"é€²åº¦: {i}/{total} ({i*100//total}%)")
        
        info = get_stock_info(code, market)
        if info:
            result = {
                'code': code,
                'name': name,
                'market': market,
                'mention_count': stock_data['mention_count'],
                'yesterday_buy': stock_data['yesterday_buy'],
                'current_price': info.get('æˆäº¤åƒ¹', '-'),
                'open_price': info.get('é–‹ç›¤åƒ¹', '-'),
                'change': info.get('æ¼²è·Œ', '-'),
                'change_percent': info.get('æ¼²è·Œå¹…', '-'),
                'buy_volume': info.get('å§”è²·å°è¨ˆ', '-'),
                'sell_volume': info.get('å§”è³£å°è¨ˆ', '-'),
                'update_time': datetime.now(TW_TZ).strftime('%Y-%m-%d %H:%M:%S')
            }
            results.append(result)
        else:
            log_warning(f"ç„¡æ³•å–å¾— {code} {name} çš„è³‡æ–™")
        
        if i < total:
            time.sleep(delay)
    
    log_success(f"å®Œæˆ {len(results)}/{total} æª”è‚¡ç¥¨è³‡è¨ŠæŠ“å–")
    return results

# ========== ä¸»ç¨‹å¼ ==========
def main():
    try:
        print("\n" + "=" * 80)
        print("å°è‚¡æ–°èç†±é–€è‚¡ç¥¨åˆ†æç³»çµ± - GitHub Actions ç‰ˆ")
        print("=" * 80)
        log_info(f"åŸ·è¡Œæ¨¡å¼: {PROCESS_MODE}")
        log_info(f"åŸ·è¡Œæ™‚é–“: {datetime.now(TW_TZ).strftime('%Y-%m-%d %H:%M:%S')}")
        log_info(f"å·¥ä½œç›®éŒ„: {os.getcwd()}")
        log_info(f"è³‡æ–™ç›®éŒ„: {BASE_PATH}")
        print("=" * 80 + "\n")
        
        # ===== æª”æ¡ˆæª¢æŸ¥ =====
        if not check_required_files():
            log_error("å¿…è¦æª”æ¡ˆæª¢æŸ¥å¤±æ•—ï¼Œç¨‹å¼çµ‚æ­¢")
            sys.exit(1)
        
        # ===== ç¬¬ä¸€éšæ®µ =====
        print("\n" + "=" * 80)
        print("[ç¬¬ä¸€éšæ®µ: çˆ¬å–æ–°è]")
        print("=" * 80)
        
        existing_news = load_existing_news(NEWS_JSON)
        new_news = scrape_twstock_news()
        
        if new_news:
            merged_news = merge_news(existing_news, new_news)
            filtered_news = filter_news_by_time(merged_news, hours=24)
            save_news_to_json(filtered_news, NEWS_JSON)
            log_success(f"æ–°èè™•ç†å®Œæˆ: æ–°æŠ“å– {len(new_news)} å‰‡, ä¿ç•™ {len(filtered_news)} å‰‡")
        else:
            filtered_news = existing_news
            log_warning(f"ä½¿ç”¨ç¾æœ‰æ–°èè³‡æ–™: {len(filtered_news)} å‰‡")
        
        # ===== ç¬¬äºŒéšæ®µ =====
        print("\n" + "=" * 80)
        print("[ç¬¬äºŒéšæ®µ: åˆ†ææ–°èè‚¡ç¥¨]")
        print("=" * 80)
        
        tse_stocks = load_stock_list(TSE_CSV, 'TSE')
        otc_stocks = load_stock_list(OTC_CSV, 'OTC')
        
        all_stocks = {}
        all_stocks.update(tse_stocks)
        all_stocks.update(otc_stocks)
        stock_mentions = analyze_news_stocks(filtered_news, tse_stocks, otc_stocks)
        
        # ===== ç¬¬ä¸‰éšæ®µ =====
        print("\n" + "=" * 80)
        print("[ç¬¬ä¸‰éšæ®µ: è¼‰å…¥è²·è¶…æ’è¡Œ]")
        print("=" * 80)
        
        tse_buy_ranking = load_buy_ranking(TSE_RANKING) if PROCESS_MODE in ['TSE', 'BOTH'] else {}
        otc_buy_ranking = load_buy_ranking(OTC_RANKING) if PROCESS_MODE in ['OTC', 'BOTH'] else {}
        
        # ===== ç¬¬å››éšæ®µ =====
        print("\n" + "=" * 80)
        print("[ç¬¬å››éšæ®µ: åˆä½µè‚¡ç¥¨]")
        print("=" * 80)
        
        tse_merged = merge_stocks(stock_mentions, tse_buy_ranking, all_stocks, 'TSE') if PROCESS_MODE in ['TSE', 'BOTH'] else {}
        otc_merged = merge_stocks(stock_mentions, otc_buy_ranking, all_stocks, 'OTC') if PROCESS_MODE in ['OTC', 'BOTH'] else {}
        
        # ===== ç¬¬äº”éšæ®µ =====
        print("\n" + "=" * 80)
        print("[ç¬¬äº”éšæ®µ: æŠ“å–è‚¡åƒ¹]")
        print("=" * 80)
        
        tse_stock_data = []
        if tse_merged and PROCESS_MODE in ['TSE', 'BOTH']:
            sorted_tse = sorted(tse_merged.values(), key=lambda x: (x['mention_count'], x['yesterday_buy']), reverse=True)
            log_info(f"æº–å‚™æŠ“å– TSE {len(sorted_tse)} æª”è‚¡ç¥¨")
            tse_stock_data = fetch_stocks_price(sorted_tse, delay=STOCK_DELAY)
        
        otc_stock_data = []
        if otc_merged and PROCESS_MODE in ['OTC', 'BOTH']:
            sorted_otc = sorted(otc_merged.values(), key=lambda x: (x['mention_count'], x['yesterday_buy']), reverse=True)
            log_info(f"æº–å‚™æŠ“å– OTC {len(sorted_otc)} æª”è‚¡ç¥¨")
            otc_stock_data = fetch_stocks_price(sorted_otc, delay=STOCK_DELAY)
        
        # ===== ç¬¬å…­éšæ®µ =====
        print("\n" + "=" * 80)
        print("[ç¬¬å…­éšæ®µ: å„²å­˜çµæœ]")
        print("=" * 80)
        
        if tse_stock_data:
            tse_output = {
                'update_time': datetime.now(TW_TZ).strftime('%Y-%m-%d %H:%M:%S'),
                'market': 'TSE',
                'total_news': len(filtered_news),
                'hot_stocks_count': len(tse_stock_data),
                'stocks': tse_stock_data
            }
            with open(TSE_OUTPUT_JSON, 'w', encoding='utf-8') as f:
                json.dump(tse_output, f, ensure_ascii=False, indent=2)
            log_success(f"TSE è³‡æ–™å·²å„²å­˜: {len(tse_stock_data)} æª”")
        
        if otc_stock_data:
            otc_output = {
                'update_time': datetime.now(TW_TZ).strftime('%Y-%m-%d %H:%M:%S'),
                'market': 'OTC',
                'total_news': len(filtered_news),
                'hot_stocks_count': len(otc_stock_data),
                'stocks': otc_stock_data
            }
            with open(OTC_OUTPUT_JSON, 'w', encoding='utf-8') as f:
                json.dump(otc_output, f, ensure_ascii=False, indent=2)
            log_success(f"OTC è³‡æ–™å·²å„²å­˜: {len(otc_stock_data)} æª”")
        
        print("\n" + "=" * 80)
        log_success("æ‰€æœ‰ä»»å‹™å®Œæˆ!")
        print("=" * 80 + "\n")
        
    except Exception as e:
        log_error(f"ç¨‹å¼åŸ·è¡Œå¤±æ•—: {e}")
        log_debug(traceback.format_exc())
        sys.exit(1)

if __name__ == "__main__":
    main()